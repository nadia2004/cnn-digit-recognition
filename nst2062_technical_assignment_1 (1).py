# -*- coding: utf-8 -*-
"""NST2062. Technical Assignment #1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18W7GdmlH1tLBCwvWy8eB796TDgiSlSqm

# Technical Assignment #1

## Problem 1: API Integration & Digit Recognition (40 points)

**Objective**:
Please load (or train) a MNIST model and integrate it with an external API (or simulated function) that supplies a handwritten digit image. You shoudl then  preprocess the image, run a prediction, and compare the model's output with the true label (which you (or an API) can provide).

**Tasks**:

1. Model Setup:

Option A: Load a provided pre-trained model.

Option B: Train a simple CNN on the MNIST dataset.

2. API/Sample Function:

Create or use a simulated API function that returns a 28x28 handwritten digit image (and its true label).

(Tip: Use random selection from the MNIST test set to simulate an API response.)

3. Prediction & Visualization:

Preprocess the received image so it matches the model's input requirements.
Use the model to predict the digit.

Display the image along with both the model's prediction and the true label.

Briefly comment on any discrepancies.

## Problem 2: Hidden Layer Visualization & Analysis (40 points)

**Objective**:
In this problem you will explore what happens “inside” the MNIST model by extracting and visualizing the activations of one or more hidden layers when processing an input image. You will then relate these observations to neurobiological ideas about feature detection in the brain.

**Tasks**:

1. Extract Intermediate Activations:

Modify the MNIST model (or create a sub-model) to output the activations from at least one convolutional layer.
Feed the digit image obtained from Problem 1 into this model.

2. Visualization:

Use visualization libraries (e.g., Matplotlib) to plot the activation maps.

Create clear subplots to show how different filters respond to the input.

3. Analysis & Neurobiological Discussion:

Discuss what kinds of features the early convolutional layers appear to detect (e.g., edges, corners).

Compare these features with what is known about the receptive fields in the human visual cortex (for example, simple cells in V1).

Write a short explanation (150–300 words) linking the role of these hidden layers to biological processes in visual perception.

## Problem 4: Adversarial Example Generation & Robustness Analysis (20 points)

**Objective**:
This advanced problem invites you to probe the robustness of your MNIST model by generating adversarial examples. You will introduce a slight, carefully crafted perturbation to an input image to cause the model to misclassify it. Finally, you will discuss the implications of such vulnerabilities and draw parallels to biological perceptual phenomena.

**Tasks**:

1. Understanding the Concept:

Read a brief introduction on adversarial examples and the Fast Gradient Sign Method (FGSM). In FGSM, small perturbations are added to the input image in the direction that increases the model's loss, potentially leading to misclassification.

2. Implementing FGSM:

Use your loaded or trained MNIST model and select an input image (from Problem 1).

Compute the gradient of the loss with respect to the input image.

Generate an adversarial perturbation by taking the sign of this gradient and multiplying it by a small factor (epsilon). For example, choose epsilon = 0.1.

Create the adversarial image by adding this perturbation to the original image.

3. Evaluating the Model:

Use your model to predict the digit for both the original and adversarial images.

Display both images side-by-side along with their predicted labels.

4. Analysis & Neurobiological Discussion:

Write a brief discussion (200–400 words) addressing:
* How a small perturbation can lead to a significant change in prediction.
* What this suggests about the model’s decision boundaries and robustness.
* Analogies to biological systems—for instance, discuss how human perception can sometimes be tricked by optical illusions or minimal changes in stimuli.
* Reflect on the potential implications for both machine learning and neuroscience research.


*Hint*: If you are using TensorFlow, you might leverage a gradient tape as follows:
"""

import tensorflow as tf
import numpy as np

# Assume 'model' is your MNIST model and 'image' is your input image with shape (1, 28, 28, 1)
image = tf.convert_to_tensor(image_for_model)

# Choose a target label if desired, or use the original label for untargeted attack
original_label = tf.convert_to_tensor([true_label])

# Set epsilon (perturbation magnitude)
epsilon = 0.1

# Use a gradient tape to compute gradients with respect to the input image
with tf.GradientTape() as tape:
    tape.watch(image)
    prediction = model(image)
    loss = tf.keras.losses.sparse_categorical_crossentropy(original_label, prediction)

# Get the gradients of the loss with respect to the input image
gradient = tape.gradient(loss, image)

# Compute the sign of the gradients to create the perturbation
perturbation = epsilon * tf.sign(gradient)

# Generate the adversarial image
adv_image = image + perturbation
adv_image = tf.clip_by_value(adv_image, 0.0, 1.0)  # Ensure the pixel values remain valid