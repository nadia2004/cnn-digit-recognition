# -*- coding: utf-8 -*-
"""Nadia_NST Tech Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cKkxQ3-_Hr6xj5r1iXRD5juzLF2njdVc
"""



"""# **Technical Assignment #1**"""

## Problem 1: API Integration & Digit Recognition (40 points)

!pip install torch torchvision pillow

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader, random_split, SubsetRandomSampler  # Import random_split
from torchvision import datasets, transforms
import torch.nn.functional as F
from PIL import Image
import matplotlib.pyplot as plt
import cv2
import os
import numpy as np

# 1. Model Definition with Adaptive Pooling
class ImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 32, (3,3)),
            nn.ReLU(),
            nn.Conv2d(32, 64, (3,3)),
            nn.ReLU(),
            nn.Conv2d(64, 64, (3,3)),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),  # Pool to 1x1 to avoid hardcoding dimensions
            nn.Flatten(),
            nn.Linear(64, 10)  # Final layer to output 10 classes (0-9)
        )

    def forward(self, x):
        return self.model(x)

# 2. Data Augmentation for Training
train_transform = transforms.Compose([
    transforms.RandomRotation(10),    # Random rotation between -10 to 10 degrees
    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Random translation
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]
])

# Load MNIST dataset
dataset = datasets.MNIST(root="data", download=True, train=True, transform=train_transform)

# Split dataset into train and validation sets (80% train, 20% validation)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)

# 3. Train the Model
def train_model(model, train_loader, val_loader, num_epochs=10):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available
    model.to(device)

    opt = Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.CrossEntropyLoss()

    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        # Training loop
        model.train()
        total_loss = 0
        for batch in train_loader:
            X, y = batch
            X, y = X.to(device), y.to(device)

            # Forward pass
            yhat = model(X)
            loss = loss_fn(yhat, y)

            # Backpropagation
            opt.zero_grad()
            loss.backward()
            opt.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader)}")

        # Validation loop
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                X, y = batch
                X, y = X.to(device), y.to(device)

                # Forward pass
                yhat = model(X)
                loss = loss_fn(yhat, y)
                val_loss += loss.item()

        print(f"Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}")

        # Save the model if validation loss improves
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_mnist_classifier.pth')
            print(f"Model saved at epoch {epoch+1}.")

# Initialize model
clf = ImageClassifier()

# Train the model with validation
train_model(clf, train_loader, val_loader)

# 4. Noise Reduction and Preprocessing for Test Image
def preprocess_image(image_path):
    # Load image using OpenCV
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Resize to 28x28 pixels to match MNIST
    img = cv2.resize(img, (28, 28))

    # Apply Gaussian blur to reduce noise
    img_blur = cv2.GaussianBlur(img, (5, 5), 0)

    # Convert to PIL Image for compatibility with torchvision transforms
    img_pil = Image.fromarray(img_blur)

    # Apply transformations: normalize same as MNIST
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    img_tensor = transform(img_pil).unsqueeze(0)  # Add batch dimension
    return img_tensor

# 5. Test on Noisy Image
def test_model_on_image(model, image_path):
    # Preprocess the noisy image
    img_tensor = preprocess_image(image_path).to('cpu')

    # Model in evaluation mode
    model.eval()
    with torch.no_grad():
        output = model(img_tensor)
        predicted = torch.argmax(output)

    # Get softmax probabilities
    probs = F.softmax(output, dim=1)
    confidence = probs[0][predicted].item()

    print(f"Predicted Label: {predicted.item()}, Confidence: {confidence}")

    # Visualize the processed image
    img_np = img_tensor.squeeze().cpu().numpy()
    plt.imshow(img_np, cmap='gray')
    plt.title(f"Predicted: {predicted.item()}, Confidence: {confidence}")
    plt.show()

# Later: Load the best model and test
clf = ImageClassifier().to('cpu')
clf.load_state_dict(torch.load('best_mnist_classifier.pth'))
print("Model loaded for inference.")

"""https://huggingface.co/immartian/improved_digits_recognition"""

import random


# Load the pre-trained model (replace 'best_mnist_classifier.pth' with your path)
model = ImageClassifier()
model.load_state_dict(torch.load('best_mnist_classifier.pth'))
model.eval()  # Set the model to evaluation mode

# MNIST Test Set
transform = transforms.Compose([transforms.ToTensor()])
mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Simulated API function to return a random MNIST digit image and its label
def get_random_mnist_image():
    random_index = random.randint(0, len(mnist_test) - 1)
    image, label = mnist_test[random_index]
    image_pil = transforms.ToPILImage()(image)
    return image_pil, label

# Get a random MNIST image and true label from the simulated API
image, true_label = get_random_mnist_image()

# Display the image
plt.imshow(image, cmap='gray')
plt.axis('off')  # Hide axis for better visualization
plt.show()

# Preprocess the image for the model
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize the image if needed
])

# Convert the image to tensor format suitable for the model
img_tensor = transform(image).unsqueeze(0)  # Add batch dimension

# Perform inference
with torch.no_grad():
    output = model(img_tensor)
    predicted_label = torch.argmax(output, dim=1).item()

# Print the true and predicted labels
print(f"True label: {true_label}")
print(f"Predicted label: {predicted_label}")